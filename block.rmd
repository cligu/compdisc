---
title: 'Blocking approach'
author: "Akitaka YAMADA"
date: \today
output: html_document
---
  
#Goal of the Project: Blocking approach
  
Goal: (i) get a textfile (.txt) and (ii) returns the topic boundary based on the blocking approach

Approach: blocking approach (Heast 1997)

```{r library}
#--------------------------------------------------
#Section 0: libraries
#--------------------------------------------------
#install.packages("qdapDictionaries")
library("qdapDictionaries")
```

First, stopwords are excluded. 

Note that I could not find any handy R NLP function that just detects the POS for each word. So, as an approximation, I removed the \textit{function word} provided in the library shown above.


```{r stopwords, ref.label="library"}
#--------------------------------------------------
#Section 1: Stop words
#--------------------------------------------------
stop.words <- function.words;length(stop.words)

```

#Defining the function

```{r blocking, cache = T, ref.label="stopwords"}
#--------------------------------------------------
#Section 2: Define the function
#--------------------------------------------------
blocking <- function(halfWindowSize
                     ,f
                     , encoding = ""
                     , plot = F
                     , smoothing = F
                     , NumOfBoundary=6
                     , main = ""
                     , stop.words=""
){
  if(stop.words=="")    library("qdapDictionaries"); stop.words <- function.words
  k = halfWindowSize
  #--------------------------------------------------
  #2-1: Load Sentences from each file
  #--------------------------------------------------
  sentences = scan(file = f, what = "char", sep = "\n", quiet = T,  encoding = encoding)
  #--------------------------------------------------
  #2-2: Frequency matrix
  #--------------------------------------------------
  #Def: mat (Matrix)
  #(a) for rows: words
  #(b) for columns: sentences
  words <- unique(tolower(unlist(strsplit(sentences, split = " "))))
  mat <- matrix(0, nrow = length(words), ncol = length(sentences));rownames(mat) <- words
  for (i in 1:length(sentences))  mat[tolower(unlist(strsplit(sentences[i], split = " "))),i] = 1
  
  #--------------------------------------------------
  #2-3: Removing the stopwords
  #--------------------------------------------------
  #Def: mat (Matrix)
  #(a) for rows: words but stopwords
  #(b) for columns: sentences
  #- - - - - - - - - - - - - - - - - - - - - - - - -   
  mat <- mat[!rownames(mat) %in% stop.words,]
  #--------------------------------------------------
  #2-4: Blocks
  #--------------------------------------------------
  #Def: block (Matrix)
  #(a) for rows: words but stopwords
  nrow = nrow(mat)
  #(b) for columns: number of blocks (= ncol)
  ncol = length(sentences) - (k-1) 
  #- - - - - - - - - - - - - - - - - - - - - - - - -   
  block <- matrix(0, nrow = nrow, ncol = ncol)
  rownames(block) <- rownames(mat)
  if(k == 1){
    for (i in 1:ncol) block[,i] <- mat[, i]
  }else{
    for (i in 1:ncol) block[,i] <- rowSums(mat[, i:(i+k-1)])
  }
  #--------------------------------------------------
  #2-5: Distance between blocks
  #--------------------------------------------------
  #[Aim] Distance are measured in terms of inner product
  #- - - - - - - - - - - - - - - - - - - - - - - - - 
  result <- vector("numeric", length = ncol-k)
  for (i in 1:(ncol-k)) result[i] = block[,i] %*% block[,i+k]
  
  #--------------------------------------------------
  #2-6: Smoothing
  #--------------------------------------------------
  #[Aim] if you specify the smoothing value, the results will get smoothed.
  #- - - - - - - - - - - - - - - - - - - - - - - - - 
  if(is.numeric(smoothing)) {
    for (i in (smoothing + 1):(length(result)-smoothing)) {
      result[i] = mean(result[(i-smoothing):(i+smoothing)])
    }
  }
  #--------------------------------------------------
  #2-7: Depth score
  #--------------------------------------------------
  #[Aim] the depth of the valley
  #- - - - - - - - - - - - - - - - - - - - - - - - - 
  depth.score <- vector("numeric", length = length(result))
  for (i in 2:(length(result)-1)) depth.score[i] <- (result[i-1] - result[i]) + (result[i+1] - result[i])
  
  #  for (i in 2:(length(result)-1)) {
  #    if((result[i-1] - result[i]) * (result[i+1] - result[i])<0) {
  #      depth.score[i] <- 0
  #    }
  #  }
  #order(depth.score, decreasing = T)[depth.score[order(depth.score, decreasing = T)]!=0]
  #--------------------------------------------------
  #2-8: Boundary Detection
  #--------------------------------------------------
  #[Aim] boundary is detected based on the septh.score
  #- - - - - - - - - - - - - - - - - - - - - - - - - 
  boundaries <- head(order(depth.score, decreasing = T), NumOfBoundary)
  
  topic.shift <- vector("numeric", length = length(sentences))
  topic.shift[1] <- 1 #the first sentence always initiates the boundary
  topic.shift[boundaries + k] <-1
  
  #--------------------------------------------------
  #2-9 Return
  #--------------------------------------------------
  return.value = list()
  return.value$topic.shift <- as.logical(topic.shift)
  return.value$block <- block
  if(plot == T) return.value$plot = plot(x=((k+1):ncol), y=result, type = "l", main = main)
  return(return.value)
  #low.6 <- head(order(result, decreasing = F))
  #high.6 <- head(order(result, decreasing = T))
  #j=1
  #s_id <- low.6[j]; sentences[s_id:(s_id+k-1)]; sentences[(s_id+k):(s_id+2*k-1)]
  #s_id <- high.6[j]; sentences[s_id:(s_id+k-1)]; sentences[(s_id+k):(s_id+2*k-1)]
}
```


```{r dir, ref.label="blocking"}
#--------------------------------------------------
#Section 3: Set the working directory
#--------------------------------------------------
setwd("C:/Users/owner/OneDrive/Documents/44_25 LING 765 Discourse Modeling/data")
files = dir(pattern = "^GUM.*txt")
```

#Example

```{r codes, ref.label="dir"}
#--------------------------------------------------
#Section 4: run the codes
#--------------------------------------------------
#[Parameters]
#(a) halfwindowSize: k (the 1/2 of the window size)
#(b)smoothing: 2 blocks
#--------------------------------------------------
#4.1: parameter setting
#--------------------------------------------------
halfWindowSize = 2
smoothing = 2
#--------------------------------------------------
#4.2: Results
#--------------------------------------------------
results <- list()#for the sentence id
results2 <- list()#for the block
par(mfrow=c(2,2))
for (i in 1:4){
  r <- blocking(halfWindowSize, files[i], encoding = "UTF-8",smoothing = smoothing , plot = T
                , main = gsub("^GUM_voyage_([^_]*?)_noheads.txt","\\1",files[i]))
  results[[i]] <- r$topic.shift
  results2[[i]] <- r$block
  ids <- seq(1:length(results[[i]]))[results[[i]]]
  abline(v = ids, lty = 2)
}
```


#Discussion



```{r, ref.label="codes"}
#--------------------------------------------------
#4.3: different different smoothing
#--------------------------------------------------
halfWindowSize = 2
results <- list()#for the sentence id
par(mfrow=c(3,4))
for (i in 1:12){
  r <- blocking(halfWindowSize, files[2], encoding = "UTF-8",smoothing = i , plot = T
                , main = paste(gsub("^GUM_voyage_([^_]*?)_noheads.txt","\\1",files[2]), "smoothing=", i ))
  results[[i]] <- r$topic.shift
  ids <- seq(1:length(results[[i]]))[results[[i]]]
  abline(v = ids, lty = 2)
}
```

The shape of the scores gives us a good shot; probably the problem coming from the idea of "valley."

```{r, ref.label="codes"}
#--------------------------------------------------
#4-4: different windwosize 
#--------------------------------------------------
smoothing = 2
results <- list()#for the sentence id
par(mfrow=c(3,4))
for (i in 1:12){
  r <- blocking(halfWindowSize = i, files[2]
                , encoding = "UTF-8",smoothing = smoothing , plot = T
                , main = paste(
                  gsub("^GUM_voyage_([^_]*?)_noheads.txt","\\1",files[2]), "smoothing=", i )
                )
  results[[i]] <- r$topic.shift
  ids <- seq(1:length(results[[i]]))[results[[i]]]
  abline(v = ids, lty = 2)
}
sentences = scan(file = files[2], what = "char", sep = "\n", quiet = T, encoding = "UTF-8")
ids <- seq(1:length(results[[3]]))[results[[3]]] # windowsize = 2
paste(ids, ": ", sentences[ids], sep = "")

```



#Interpretation

```{r sentences, ref.label="codes"}
#--------------------------------------------------
#Section 5: Each document 
#--------------------------------------------------
#--------------------------------------------------
#5.1: Athens
#--------------------------------------------------
#  results2[[1]][,]
#--------------------------------------------------
#5.1: Athens
#--------------------------------------------------
sentences = scan(file = files[2], what = "char", sep = "\n", quiet = T, encoding = "UTF-8")
ids <- seq(1:length(results[[2]]))[results[[2]]]
paste(ids, ": ", sentences[ids], sep = "")

```

